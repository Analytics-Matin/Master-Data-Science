---
title: "Aprendizaje, Generalización y Sobreajuste"
subtitle: "Validación cruzada"
author: "Grupo de Meteorología"
output:
  html_document:
    highlight: textmate
    mathjax: https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_depth: 4
---



# Modelos de aprendizaje

Para que un problema sea adecuado para su resolución mediante técnicas de aprendizaje automático la condición fundamental es la existencia de datos para alimentar (entrenar) el algoritmo de aprendizaje.

De esta forma, a partir de un conjunto de datos de entrenamiento que se considera representativo de la distribución que se quiere modelar, nuestro modelo de aprendizaje viene dado por:

* El **algoritmo usado** para aprender el patrón y 
* La familia de **funciones** 
* Los **datos** que utilicemos para aproximar el patrón a aprender. 

De este modo, el proceso de aprendizaje dependerá de estos tres elementos, los cuales establecerán nuestras limitaciones e incertidumbres a la hora tanto de aprender el patrón como de realizar predicciones con el patrón aprendido.

# Generalización y sobreajuste

Recordemos que el objetivo principal del modelo aprendido es que tenga **la capacidad de generalizar**, es decir, la capacidad de funcionar bien para nuevos datos que no forman parte de la muestra de entrenamiento (por ejemplo, una muestra de datos de test). En caso contario, diremos que el modelo está **sobreajustado a la muestra de entrenamiento**. 

**La introducción de grados de libertad en la familia de funciones consideradas en el aprendizaje suele dar lugar a modelos sobreajustados**, por lo que suele ser conveniente partir de los modelos más simples e ir introduciendo grados de libertad progresivamente si fuera necesario.

Teniendo en cuenta que sólo podemos evaluar el error de nuestro modelo aprendido en la muestra de entrenamiento, **¿cómo analizar el sobreajuste del modelo?**

Mediante **técnicas de validación cruzada y técnicas de remuestreo o bootstraping**

La validación cruzada se basa en separar la muestra en dos conjuntos disjuntos, uno de entrenamiento y otro de test, que permita analizar tanto el error muestral como la capacidad de generalización de nuestro modelo aprendido en la fase de entrenamiento a través del conjunto de test. 

En esta práctica utilizaremos...

* modelo: Regresion lineal
* Dataset: Auto (paquete ISLR)
* Validación: error medio absoluto (MAE) y error cuadrático medio (MSE)
* Librerías de R:
```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
library(ISLR)
library(caret)
```

# Carga y transformación de datos

El dataset Auto (paquete ISLR) contiene información sobre las características de 392 vehículos.

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
data(Auto)
str(Auto)
```

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
?Auto
# Conversion de libras a Kg
Auto$weight <- Auto$weight * 0.453592
```

Comenzamos con un análisis preliminar de nuestros datos:

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
pairs(Auto)
```

En este ejemplo solo nos interesa la relación entre el peso (weight) y la potencia (horsepower).

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
pairs(weight ~ horsepower, data = Auto, col = c("orange", "green3", "blue")[unclass(Auto$origin)])
```

Imaginemos que una empresa X nos proporciona únicamente los registros o datos europeos para que obtengamos un modelo capaz de estimar los pesos de los coches americanos en función de la potencia.

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
Auto$origin <- as.character(
                  factor(Auto$origin, 
                         labels = c("American", "Japanese", "European")))
# muestra de test
america <- Auto[Auto$origin == "American", ]
# muestra de train
europe <- Auto[Auto$origin == "European", ]
```

Visualicemos ambos conjuntos:

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
plot(america$horsepower, america$weight, 
     xlab = "horsepower", ylab = "weight",
     pch = 16, col = "red",
     xlim = range(Auto$horsepower))
points(europe$horsepower, europe$weight, pch = 16)
# La función `lm` realiza el ajuste de un modelo lineal entre ambas variables:
abline(lm(weight~horsepower, data = america), col = "red")
abline(lm(weight~horsepower, data = europe))
```


# Definición de las funciones de error

Este modo de definir funciones es bastante útil en R. 

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
# Functions
mae <- function(obs, est) {
  mean(abs(obs - est))
}

rmse <- function(obs, est) {
  sqrt(mean((obs - est)^2))
}
```


# Modelización y evaluación

La función `lm` realiza el ajuste de un modelo lineal entre ambas variables:

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
# Salida de la regresion
reg <- lm(weight~horsepower, data = europe) 
summary(reg)
```


Podemos extraer los "fitted values" de `reg`, es decir, el resultado de la applicación del modelo ajustado a los propios datos de entrenamiento `yest`, para comparar la estimación (o predicción) del modelo con los datos reales (observados).

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
yest <- reg$fitted.values
```

---Nota:
Lo que obtenemos en la linea anterior es lo mismo que utilizar nuestro modelo para predecir el weight en base a la horsepower que se utilizo en el proceso de entrenamiento:
```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
yest <- predict(object = reg, newdata = data.frame(horsepower = europe$horsepower))
```
---

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
plot(europe$weight, type = 'l')
lines(yest, col = 'blue')
```

Por último calculamos el MAE y el MSE:

```{r}
mae(obs = europe$weight, yest)
rmse(obs = europe$weight, yest)
```
```{r}
cor(europe$weight, yest, method = "spearman")
```
El ejercicio anterior utiliza todos los datos para entrenar y validar el modelo. Es decir, la validación se realiza utilizando como conjunto de validación el mismo conjunto utilizado como "train" (conjunto de entrenamiento). Por lo tanto no podemos estimar la capacidad de generalización o sobreajuste del modelo. 

Si la validación se realiza sobre un conjunto independiente de la muestra de entrenamiento el error esperable es mayor.

## Modelización y validación hold out 

El modo más básico de analizar el error de mi modelo es dividiendo la muestra en subconjuntos disjuntos (hold out). En este primer caso consideraremos sólo dos conjuntos, uno de train y otro de validación con la mitad de datos (instancias, i.e. filas) en cada uno. 

```{r, echo=TRUE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
n <- nrow(europe)
ind <- order(europe$horsepower)[1:ceiling(n/2)]
europe.train <- europe[ind, ]
europe.val <- europe[-ind,]

plot(america$horsepower, america$weight, 
     xlab = "horsepower", ylab = "weight",
     pch = 16, col = "red",
     xlim = range(Auto$horsepower))
points(europe.train$horsepower, europe.train$weight, pch = 16, col = "black")
points(europe.val$horsepower, europe.val$weight, pch = 21, bg = "white", cex = 0.8)

```

Aplicamos de nuevo la funciones `lm` (para ajustar el modelo) y  `predict` (para aplicar el modelo sobre los datos de "train" y de "validación") y calculamos el RMSE:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
reg0 <- lm(weight~horsepower, data = europe.train)
yest0.train <- predict(reg0, newdata = data.frame(horsepower = europe.train$horsepower))
yest0.val <- predict(reg0, newdata = data.frame(horsepower = europe.val$horsepower))
rmse(europe.train$weight, yest0.train)
rmse(europe.val$weight, yest0.val)
```
Hemos entrenado con los valores de horsepower menores, por lo que **nuestra muestra no es representativa de la población**. 

### Muestreo aleatiorio para obtener las muestras de train y de validación

Para intentar minimizar el sesgo de nuestra muestra de entrenamiento, una solución es aleatorizar la selección. En este ejemplo utilizamos la función `sample` para obtener un índice de registros aleatoria. 

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
n <- nrow(europe)
set.seed(1)
ind <- sample(1:n, ceiling(n/2))
europe.train <- europe[ind, ]
europe.val <- europe[-ind,]

plot(america$horsepower, america$weight, 
     xlab = "horsepower", ylab = "weight",
     pch = 16, col = "red",
     xlim = range(Auto$horsepower))
points(europe.train$horsepower, europe.train$weight, pch = 16, col = "black")
points(europe.val$horsepower, europe.val$weight, pch = 21, bg = "white", cex = 0.8)
```

Aplicamos de nuevo la funciones `lm` (para ajustar el modelo) y  `predict` (para aplicar el modelo sobre los datos de "train" y de "validación") y calculamos el RMSE:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

reg1 <- lm(weight~horsepower, data = europe.train)
yest1.train <- predict(reg1, newdata = data.frame(horsepower = europe.train$horsepower))
yest1.val <- predict(reg1, newdata = data.frame(horsepower = europe.val$horsepower))
rmse(europe.train$weight, yest1.train)
rmse(europe.val$weight, yest1.val)
```

Al considerar una muestra representativa de la variabilidad de la población el error de validación es más bajo y se asemeja más al error de train que en el ejemplo anterior.

**El modelo entrenado con una muestra aleatoria de `europe` tiene más capacidad de generalización** 

Ya que ...

Decimos que existe sobreajuste cuando el error de train y el de validación son muy diferentes. Un modelo con capacidad de generalización, no sobreajustado, es aquel para el que ambos errores, en la muestra de entrenamiento y de test, son similares/comparables. 


A pesar de ello, esta metodología tiene dos inconvenientes potenciales:

* 1.- La estimación del error en el conjunto de test puede variar mucho en función de la partición considerada.

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
plot(europe$horsepower, europe$weight, pch = "*")
for (i in c(1:5)) {
  ind <- sample(1:n, ceiling(n/2))
  europe.train.i <- europe[ind, ]
  europe.val.i <- europe[-ind, ]
  reg.i <- lm(weight~horsepower, data = europe.train.i)
  yest.val.i <- predict(reg.i, data.frame(horsepower = europe.val.i$horsepower))
  abline(reg.i)
  print(rmse(europe.val.i$weight, yest.val.i))
}
```

* 2.- El error de validación puede ser sobreestimado. Dado que el modelo se entrena en un subconjunto de la muestra y que los métodos estadísticos suelen comportarse peor cuando son entrenados con pocos datos (n/2 = 40 en los ejemplos anteriores), esto puede dar lugar a una sobrestimación del error de validación respecto al obtenido considerando toda la muestra.

La validación cruzada (cross-validación) que aplicaremos un poco más adelante considera estos dos problemas. 

## Modelización y cross-validación leave-one-out

Como alternativa al muestreo aleatorio existe el método de validación cruzada denominado leave-one-out:

* La selección de la muestra de entrenamiento NO se hace aleatoriamente, eliminando la variabilidad del error de validación.

* La muestra de entrenamiento es la mayor posible que considera una muestra de validación.

* Si un conjunto de datos tiene `N` registros, el ajuste del modelo se realiza con `N - 1` registros y el registro no considerado en el conjunto de entrenamiento se utiliza como validación o muestra independiente para validar el modelo.

* Esta operación se repite `N` veces, así todos los registros del dataset se utilizan como dato de test para un modelo entrenado con el resto de registros.


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
ind <- 1:n
yest.2 <- numeric(length = length(ind))

for (i in ind) {
  Reg.i <- lm(weight~horsepower, data = europe, subset = ind[-i])
  yest.2[i] <- predict(Reg.i, data.frame(horsepower = europe$horsepower[i]))
}
rmse(europe$weight, yest.2)
```

### ejemplo anterior con la librería caret

```{r}
ctrl <- trainControl(method = "LOOCV")
mod <- train(weight ~ horsepower,
               data = europe,
               method = "lm",
               trControl = ctrl)
mod
```
```{r}
mod$results$RMSE
```

## Modelización y cross-validación k-fold


Si el tamaño muestral es grande el método leave-one-out es computacionalmente costoso. Para evitar este coste surge otro método de validación cruzada: **El método k-fold** en el que se hace un leave-one-out por "bloques" o "folds": 

* se divide la muestra en `k` subconjuntos.

* Se ajustan `k` modelos, considerando en cada caso un bloque como conjunto de validación y los `k-1` restantes como muestra de entrenamiento. 

* La estimación dependerá de como se realice la partición de los datos. La variabilidad mayor que en el caso del leave-one-out.

* Con un número suficiente de subconjuntos, se obtienen los mismos resultados y conclusiones que las obtenidas con un leave-one-out. 

Consideramos el ejemplo anterior con 10 subconjuntos y vamos paso a paso.

1) Dividimos la muestra en 10 subconjuntos (`k = 10`)

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
k <- 10
# Número de registros (instancias u observaciones) en nuestro dataset.
n <- nrow(europe) 
# factor de números aleatorios con k levels (del 1 al 10) 
set.seed(1)
split.factor <- sample(rep(1:10, each = ceiling(n/k)), n) 
# Lista que en cada "slot" contiene un fold 
spl.europe <- split(europe, f = split.factor)
str(spl.europe)
```

2) ajustamos el modelo con la función `lm` con `k-1` y predecimos (función `predict`) sobre el fold restante. Se repite la operación `k` veces (en un bucle lapply):

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
yest.3 <- lapply(1:k, function(x) {
  reg.3 <- lm(weight~horsepower, data = do.call("rbind", spl.europe[-x]))
  predict(reg.3, data.frame(horsepower = spl.europe[[x]]$horsepower))
})
str(yest.3)
# Hacemos un plot de los valores de weight rales frente a los estimados 
plot(do.call("rbind", spl.europe)$weight, typ = "l")
lines(do.call("c", yest.3), col = "red")
```

3) Calculamos el error

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# de cada fold
rmse.val.3.folds <- lapply(1:length(spl.europe), function(x) rmse(spl.europe[[x]]$weight, yest.3[[x]]))
# de la muestra entera
rmse.val.3 <- rmse(do.call("rbind", spl.europe)$weight, do.call("c", yest.3))
plot(do.call("c", rmse.val.3.folds), ylab = "rmse", xlab = "fold", pch = "x", ylim = c(0, 400), col = "red")
abline(h = rmse.val.3, col = "red")
abline(h = rmse(obs = europe$weight, yest))
```


# Práctica 1

¿Es el modelo aprendido con los registros europeos adecuado para estimar el peso de los coches americanos? (¿qué ocurre cuando aplicamos el modelo a la muestra de test (objeto `america`)?)

Utiliza las funciones de visualización (`plot`, `points`, `lines`, `abline`, ...) para ilustrar los resultados.

Escribe el código a continuación:
```{r, echo=FALSE}
reg.3.all <- lm(weight~horsepower, data = europe)
yest.test.3 <- predict(reg.3.all, newdata = data.frame(horsepower = america$horsepower))
plot(do.call("c", rmse.val.3.folds), ylab = "rmse", xlab = "fold", pch = "x", ylim = c(0, 400))
abline(h = rmse.val.3)
abline(h = rmse(america$weight, yest.test.3), col = "red")
```

```{r}
reg.3.all <- lm(weight~horsepower, data = europe)

#(...)

plot(do.call("c", rmse.val.3.folds), ylab = "rmse", xlab = "fold", pch = "x", ylim = c(0, 400))
abline(h = rmse.val.3)
#abline(...

#(...)
```


# Práctica 2

Utiliza la librería caret para reproducir el último ejemplo de cross-validación con k-fold y obten el error de validación global.

Escribe el código a continuación:

```{r, echo=FALSE}
ctrl <- trainControl(method = "cv", number = 10)
mod <- train(weight ~ horsepower,
               data = europe,
               method = "lm",
               trControl = ctrl,
               metric = "RMSE")
              
```

```{r}
```


# Práctica 3

Utiliza los datos de los coches americanos para estimar los pesos de los coches europeos por un lado y la de los japoneses por otro.

¿Es el modelo aprendido adecuado para estimar el peso de los coches europeos y japoneses?

Utiliza caret y las funciones de visualización (`plot`, `points`, `lines`, `abline`, ...) para ilustrar los resultados. 

Escribe el código a continuación:


```{r, echo=FALSE}
japan <- Auto[Auto$origin == "Japanese", ]

ctrl <- trainControl(method = "cv", number = 10)
mod <- train(weight ~ horsepower,
               data = america,
               method = "lm",
               trControl = ctrl,
               metric = "RMSE")
mod
yest.japan <- predict(mod, data.frame(horsepower = japan$horsepower))
yest.europe <- predict(mod, data.frame(horsepower = europe$horsepower))
rmse.japan <- rmse(japan$weight, yest.japan)
rmse.europe <- rmse(europe$weight, yest.europe)

plot(do.call("c", rmse.val.3.folds), ylab = "rmse", xlab = "fold", pch = "x", ylim = c(0, 400))
plot.new()
plot.window(xlim = c(0,1), ylim = c(0, 400))
axis(1, at = seq(0, 1, 1))
axis(2, at = seq(0, 400, 20))
abline(h = mod$results$RMSE)
abline(h = rmse.japan, col = "deeppink")
abline(h = rmse.europe, col = "blue")
```

# Práctica 4


# Session Info:

```{r, echo=FALSE, eval=TRUE, warning=FALSE}
print(sessionInfo())
```
